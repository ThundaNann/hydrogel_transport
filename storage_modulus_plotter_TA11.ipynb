{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c637e90-a305-493f-a044-b12fd446fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected directory: C:/Users/Instrument_Room/OneDrive - UCB-O365 (1)/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Rheometer experiments/Fall Semester 2026\n"
     ]
    }
   ],
   "source": [
    "# === Block 1: Directory Selection ===\n",
    "# Run this first to select the directory containing your Excel files\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "\n",
    "def select_directory():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes('-topmost', True)\n",
    "    directory_path = filedialog.askdirectory(\n",
    "        title=\"Select Directory Containing Excel Files\"\n",
    "    )\n",
    "    root.destroy()\n",
    "    if not directory_path:\n",
    "        raise FileNotFoundError(\"No directory selected.\")\n",
    "    return directory_path\n",
    "\n",
    "# Run the directory selector\n",
    "base_directory = select_directory()\n",
    "print(\"Selected directory:\", base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70e7f8e9-6a95-4bb3-af38-4b9d3f172fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to short path: C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\n",
      "\n",
      "\n",
      "Found 108 CSV files:\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\0 cycles\\D_1X PBS_0 cycles_1.csv\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\0 cycles\\D_1X PBS_0 cycles_2.csv\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\0 cycles\\D_1X PBS_0 cycles_3.csv\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\0 cycles\\F_1X PBS_0 cycles_1.csv\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\0 cycles\\F_1X PBS_0 cycles_2.csv\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\0 cycles\\F_1X PBS_0 cycles_3.csv\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\28 cycles\\D_1X PBS_28 cycles_1.csv\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\28 cycles\\D_1X PBS_28 cycles_2.csv\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\28 cycles\\D_1X PBS_28 cycles_3.csv\n",
      "  - C:/Users/INSTRU~1/ONEDRI~2/Courses/YEAR1~1/FALLSE~1/CHEN58~1/HYDROG~1/RHEOME~1/FALLSE~1\\06OCT25\\2% gelatin\\1X PBS\\28 cycles\\F_1X PBS_28 cycles_1.csv\n",
      "  ... and 98 more\n",
      "\n",
      "\n",
      "Processing 108 CSV files...\n",
      "Calculating average modulus in frequency window: 0.3-1.0 rad/s\n",
      "\n",
      "    âœ“ D_1X PBS_0 cycles_1.csv: avg = 79.3 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_0 cycles_2.csv: avg = 95.8 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_0 cycles_3.csv: avg = 97.0 Pa (0 cycles) [Distance]\n",
      "    âœ“ F_1X PBS_0 cycles_1.csv: avg = 70.1 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_0 cycles_2.csv: avg = 135.3 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_0 cycles_3.csv: avg = 115.2 Pa (0 cycles) [Focus]\n",
      "    âœ“ D_1X PBS_28 cycles_1.csv: avg = 79.8 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_28 cycles_2.csv: avg = 57.1 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_28 cycles_3.csv: avg = 132.8 Pa (28 cycles) [Distance]\n",
      "    âœ“ F_1X PBS_28 cycles_1.csv: avg = 75.3 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_28 cycles_2.csv: avg = 98.5 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_28 cycles_3.csv: avg = 86.3 Pa (28 cycles) [Focus]\n",
      "    âœ“ D_DBPC HMSN_0 cycles_1.csv: avg = 130.1 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_0 cycles_2.csv: avg = 114.3 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_0 cycles_3.csv: avg = 88.1 Pa (0 cycles) [Distance]\n",
      "    âœ“ F_DBPC HMSN_0 cycles_1.csv: avg = 106.4 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_0 cycles_2.csv: avg = 80.5 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_0 cycles_3.csv: avg = 81.6 Pa (0 cycles) [Focus]\n",
      "    âœ“ D_DBPC HMSN_28 cycles_1.csv: avg = 116.8 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_28 cycles_2.csv: avg = 88.7 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_28 cycles_3.csv: avg = 108.6 Pa (28 cycles) [Distance]\n",
      "    âœ“ F_DBPC HMSN_28 cycles_1.csv: avg = 82.4 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_28 cycles_2.csv: avg = 33.3 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_28 cycles_3.csv: avg = 59.4 Pa (28 cycles) [Focus]\n",
      "    âœ“ D_MSN_0 cycles_1.csv: avg = 91.2 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_MSN_0 cycles_2.csv: avg = 97.1 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_MSN_0 cycles_3.csv: avg = 80.0 Pa (0 cycles) [Distance]\n",
      "    âœ“ F_MSN_0 cycles_1.csv: avg = 97.9 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_MSN_0 cycles_2.csv: avg = 85.2 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_MSN_0 cycles_3.csv: avg = 93.7 Pa (0 cycles) [Focus]\n",
      "    âœ“ D_MSN_28 cycles_1.csv: avg = 75.9 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_MSN_28 cycles_2.csv: avg = 36.6 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_MSN_28 cycles_3.csv: avg = 49.2 Pa (28 cycles) [Distance]\n",
      "    âœ“ F_MSN_28 cycles_1.csv: avg = 106.3 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_MSN_28 cycles_2.csv: avg = 145.0 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_MSN_28 cycles_3.csv: avg = 68.8 Pa (28 cycles) [Focus]\n",
      "    âœ“ D_1X PBS_0 cycles_1.csv: avg = 403.7 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_0 cycles_2.csv: avg = 482.5 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_0 cycles_3.csv: avg = 438.9 Pa (0 cycles) [Distance]\n",
      "    âœ“ F_1X PBS_0 cycles_1.csv: avg = 452.9 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_0 cycles_2.csv: avg = 540.2 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_0 cycles_3.csv: avg = 372.1 Pa (0 cycles) [Focus]\n",
      "    âœ“ D_1X PBS_28 cycles_1.csv: avg = 412.7 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_28 cycles_2_redo.csv: avg = 290.5 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_28 cycles_3.csv: avg = 250.3 Pa (28 cycles) [Distance]\n",
      "    âœ“ F_1X PBS_28 cycles_1.csv: avg = 423.9 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_28 cycles_2_redo.csv: avg = 600.5 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_28 cycles_3.csv: avg = 472.3 Pa (28 cycles) [Focus]\n",
      "    âœ“ D_DBPC HMSN_0 cycles_1.csv: avg = 439.8 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_0 cycles_2.csv: avg = 631.0 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_0 cycles_3.csv: avg = 429.8 Pa (0 cycles) [Distance]\n",
      "    âœ“ F_DBPC HMSN_0 cycles_1.csv: avg = 504.3 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_0 cycles_2.csv: avg = 360.7 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_0 cycles_3.csv: avg = 439.3 Pa (0 cycles) [Focus]\n",
      "    âœ“ D_DBPC HMSN_28 cycles_1.csv: avg = 328.4 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_28 cycles_2.csv: avg = 335.4 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_28 cycles_3.csv: avg = 360.7 Pa (28 cycles) [Distance]\n",
      "    âœ“ F_DBPC HMSN_28 cycles_1.csv: avg = 289.8 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_28 cycles_2.csv: avg = 368.4 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_28 cycles_3.csv: avg = 295.0 Pa (28 cycles) [Focus]\n",
      "    âœ“ D_MSN_0 cycles_1.csv: avg = 373.7 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_MSN_0 cycles_2.csv: avg = 617.3 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_MSN_0 cycles_3_redo.csv: avg = 479.0 Pa (0 cycles) [Distance]\n",
      "    âœ“ F_MSN_0 cycles_1.csv: avg = 463.7 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_MSN_0 cycles_2.csv: avg = 435.6 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_MSN_0 cycles_3_redo.csv: avg = 448.7 Pa (0 cycles) [Focus]\n",
      "    âœ“ D_MSN_28 cycles_2.csv: avg = 364.1 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_MSN_28 cycles_3.csv: avg = 296.4 Pa (28 cycles) [Distance]\n",
      "    âœ“ D_MSN_28 cycles_4.csv: avg = 457.5 Pa (28 cycles) [Distance]\n",
      "    âœ“ F_MSN_28 cycles_2.csv: avg = 381.5 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_MSN_28 cycles_3.csv: avg = 355.2 Pa (28 cycles) [Focus]\n",
      "    âœ“ F_MSN_28 cycles_4.csv: avg = 327.9 Pa (28 cycles) [Focus]\n",
      "    âœ“ D_1X PBS_0 cycles_1.csv: avg = 2560.3 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_0 cycles_2.csv: avg = 1968.1 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_0 cycles_3.csv: avg = 2519.5 Pa (0 cycles) [Distance]\n",
      "    âœ“ F_1X PBS_0 cycles_1.csv: avg = 2057.3 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_0 cycles_2.csv: avg = 2006.0 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_0 cycles_3.csv: avg = 2612.9 Pa (0 cycles) [Focus]\n",
      "    âœ“ D_1X PBS_40 cycles_1.csv: avg = 1423.8 Pa (40 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_40 cycles_3.csv: avg = 1415.6 Pa (40 cycles) [Distance]\n",
      "    âœ“ D_1X PBS_40 cycles_4.csv: avg = 1376.5 Pa (40 cycles) [Distance]\n",
      "    âœ“ F_1X PBS_40 cycles_1.csv: avg = 1492.8 Pa (40 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_40 cycles_3.csv: avg = 1673.6 Pa (40 cycles) [Focus]\n",
      "    âœ“ F_1X PBS_40 cycles_4.csv: avg = 1834.7 Pa (40 cycles) [Focus]\n",
      "    âœ“ D_DBPC HMSN_0 cycles_1.csv: avg = 2518.0 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_0 cycles_2.csv: avg = 2509.6 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_0 cycles_3.csv: avg = 1764.4 Pa (0 cycles) [Distance]\n",
      "    âœ“ F_DBPC HMSN_0 cycles_1.csv: avg = 2608.9 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_0 cycles_2.csv: avg = 2588.3 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_0 cycles_3.csv: avg = 2680.8 Pa (0 cycles) [Focus]\n",
      "    âœ“ D_DBPC HMSN_40 cycles_1.csv: avg = 2535.7 Pa (40 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_40 cycles_2.csv: avg = 1936.1 Pa (40 cycles) [Distance]\n",
      "    âœ“ D_DBPC HMSN_40 cycles_3.csv: avg = 1899.2 Pa (40 cycles) [Distance]\n",
      "    âœ“ F_DBPC HMSN_40 cycles_1.csv: avg = 1322.9 Pa (40 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_40 cycles_2.csv: avg = 854.2 Pa (40 cycles) [Focus]\n",
      "    âœ“ F_DBPC HMSN_40 cycles_3.csv: avg = 1461.8 Pa (40 cycles) [Focus]\n",
      "    âœ“ D_MSN_0 cycles_1.csv: avg = 2290.9 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_MSN_0 cycles_2.csv: avg = 1937.3 Pa (0 cycles) [Distance]\n",
      "    âœ“ D_MSN_0 cycles_3.csv: avg = 2769.0 Pa (0 cycles) [Distance]\n",
      "    âœ“ F_MSN_0 cycles_1.csv: avg = 2494.8 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_MSN_0 cycles_2.csv: avg = 1809.6 Pa (0 cycles) [Focus]\n",
      "    âœ“ F_MSN_0 cycles_3.csv: avg = 2186.4 Pa (0 cycles) [Focus]\n",
      "    âœ“ D_MSN_40 cycles_1.csv: avg = 858.1 Pa (40 cycles) [Distance]\n",
      "    âœ“ D_MSN_40 cycles_2_redo.csv: avg = 1491.9 Pa (40 cycles) [Distance]\n",
      "    âœ“ D_MSN_40 cycles_3.csv: avg = 927.5 Pa (40 cycles) [Distance]\n",
      "    âœ“ F_MSN_40 cycles_1.csv: avg = 1631.3 Pa (40 cycles) [Focus]\n",
      "    âœ“ F_MSN_40 cycles_2_redo.csv: avg = 1468.1 Pa (40 cycles) [Focus]\n",
      "    âœ“ F_MSN_40 cycles_3.csv: avg = 1586.0 Pa (40 cycles) [Focus]\n",
      "\n",
      "============================================================\n",
      "SUMMARY:\n",
      "  Successfully read: 108/108 files\n",
      "  Failed to read: 0/108 files\n",
      "  Global Y-max (based on averages): 2769.0 Pa\n",
      "============================================================\n",
      "\n",
      "\n",
      "Global Y-max to use for all plots: 2769.0 Pa\n"
     ]
    }
   ],
   "source": [
    "# === Block 2 Fixed: Find CSV Files and Calculate Global Y-Max ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert base_directory to short path format to handle OneDrive issues\n",
    "try:\n",
    "    import win32api\n",
    "    base_directory_short = win32api.GetShortPathName(base_directory)\n",
    "    print(f\"Converted to short path: {base_directory_short}\\n\")\n",
    "    base_directory = base_directory_short\n",
    "except:\n",
    "    print(\"Note: Could not convert to short path format\")\n",
    "    print(\"If you get file not found errors, manually set base_directory to short path format\\n\")\n",
    "\n",
    "def find_csv_files(directory):\n",
    "    \"\"\"Recursively find all CSV files in directory and subdirectories, excluding 'old' folders.\"\"\"\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Skip 'old' directories\n",
    "        if 'old' in [d.lower() for d in root.split(os.sep)]:\n",
    "            continue\n",
    "            \n",
    "        for file in files:\n",
    "            if file.endswith('.csv') and not file.startswith('~$'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return sorted(csv_files)\n",
    "\n",
    "def detect_cycle_in_filename(filename):\n",
    "    \"\"\"Detect which cycle type is in the filename.\"\"\"\n",
    "    lower = filename.lower()\n",
    "    \n",
    "    if '40 cycles' in lower or '40cycles' in lower:\n",
    "        return '40 cycles', '3.6%'\n",
    "    elif '28 cycles' in lower or '28cycles' in lower:\n",
    "        return '28 cycles', '2.5%'\n",
    "    elif '0 cycles' in lower or '0cycles' in lower:\n",
    "        return '0 cycles', '0%'\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def detect_focus_distance_filename(filename):\n",
    "    \"\"\"Detect whether file is Focus (F) or Distance (D) measurement.\"\"\"\n",
    "    basename = os.path.basename(filename).upper()\n",
    "    \n",
    "    if basename.startswith('F_'):\n",
    "        return 'Focus'\n",
    "    elif basename.startswith('D_'):\n",
    "        return 'Distance'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_gelatin_concentration(file_path):\n",
    "    \"\"\"Extract gelatin concentration from file path.\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'(\\d+)%\\s*gelatin', file_path, re.IGNORECASE)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}%\"\n",
    "    return None\n",
    "\n",
    "def calculate_global_ymax_from_csv(csv_files, freq_window=(0.3, 1.0)):\n",
    "    \"\"\"Calculate the maximum AVERAGE storage modulus across all CSV files.\"\"\"\n",
    "    global_max_avg = 0\n",
    "    successful_reads = 0\n",
    "    failed_reads = 0\n",
    "    \n",
    "    print(f\"Processing {len(csv_files)} CSV files...\")\n",
    "    print(f\"Calculating average modulus in frequency window: {freq_window[0]}-{freq_window[1]} rad/s\\n\")\n",
    "    \n",
    "    for csv_path in csv_files:\n",
    "        filename = os.path.basename(csv_path)\n",
    "        lower = filename.lower()\n",
    "        \n",
    "        # Check if file contains a cycle type\n",
    "        cycle_type, duty_cycle = detect_cycle_in_filename(filename)\n",
    "        has_cycle = cycle_type is not None\n",
    "        \n",
    "        # Check for particles\n",
    "        has_particle = any(p in lower for p in ['dbpc hmsn', 'dbpc_hmsn', 'msn', 'pbs'])\n",
    "        \n",
    "        # Detect focus/distance\n",
    "        focus_type = detect_focus_distance_filename(filename)\n",
    "        \n",
    "        if has_cycle and has_particle:\n",
    "            try:\n",
    "                # Skip first 53 rows (metadata + headers + units)\n",
    "                # Data starts at line 54\n",
    "                df = pd.read_csv(csv_path, skiprows=53, header=None)\n",
    "                \n",
    "                # Column 0 = Angular frequency\n",
    "                # Column 6 = Storage modulus\n",
    "                if len(df) > 0 and len(df.columns) > 6:\n",
    "                    # Get frequency and storage modulus data\n",
    "                    freq_data = df.iloc[:, 0]\n",
    "                    modulus_data = df.iloc[:, 6]\n",
    "                    \n",
    "                    # Create a clean dataframe\n",
    "                    data = pd.DataFrame({\n",
    "                        'Angular Frequency': pd.to_numeric(freq_data, errors='coerce'),\n",
    "                        'Storage Modulus': pd.to_numeric(modulus_data, errors='coerce')\n",
    "                    }).dropna()\n",
    "                    \n",
    "                    if not data.empty:\n",
    "                        # Filter for frequency window\n",
    "                        mask = (data['Angular Frequency'] >= freq_window[0]) & \\\n",
    "                               (data['Angular Frequency'] <= freq_window[1])\n",
    "                        window_data = data[mask]\n",
    "                        \n",
    "                        if not window_data.empty:\n",
    "                            # Calculate average in window\n",
    "                            avg_modulus = window_data['Storage Modulus'].mean()\n",
    "                            global_max_avg = max(global_max_avg, avg_modulus)\n",
    "                            \n",
    "                            focus_label = f\" [{focus_type}]\" if focus_type else \"\"\n",
    "                            print(f\"    âœ“ {filename}: avg = {avg_modulus:.1f} Pa ({cycle_type}){focus_label}\")\n",
    "                            successful_reads += 1\n",
    "                        else:\n",
    "                            print(f\"    âš  {filename}: no data in frequency window\")\n",
    "                            failed_reads += 1\n",
    "                    else:\n",
    "                        print(f\"    âš  {filename}: no valid numeric data\")\n",
    "                        failed_reads += 1\n",
    "                else:\n",
    "                    print(f\"    âš  {filename}: insufficient data\")\n",
    "                    failed_reads += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    âœ— {filename}: Error - {str(e)[:100]}\")\n",
    "                failed_reads += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY:\")\n",
    "    print(f\"  Successfully read: {successful_reads}/{len(csv_files)} files\")\n",
    "    print(f\"  Failed to read: {failed_reads}/{len(csv_files)} files\")\n",
    "    print(f\"  Global Y-max (based on averages): {global_max_avg:.1f} Pa\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return global_max_avg\n",
    "\n",
    "# Find all CSV files\n",
    "csv_files = find_csv_files(base_directory)\n",
    "print(f\"\\nFound {len(csv_files)} CSV files:\")\n",
    "\n",
    "# Show first 10 files as preview\n",
    "for f in csv_files[:10]:\n",
    "    print(f\"  - {f}\")\n",
    "if len(csv_files) > 10:\n",
    "    print(f\"  ... and {len(csv_files) - 10} more\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Calculate global maximum based on averages\n",
    "if csv_files:\n",
    "    global_ymax = calculate_global_ymax_from_csv(csv_files)\n",
    "    print(f\"\\nGlobal Y-max to use for all plots: {global_ymax:.1f} Pa\")\n",
    "else:\n",
    "    print(\"\\nâš  WARNING: No CSV files found!\")\n",
    "    print(\"Check that your base_directory is correct and contains CSV files\")\n",
    "    global_ymax = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11eabc2a-048a-4b4b-82c4-835ce1f97d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base directory for outputs: C:\\Users\\INSTRU~1\\ONEDRI~2\\Courses\\YEAR1~1\\FALLSE~1\\CHEN58~1\\HYDROG~1\\RHEOME~1\\FALLSE~1\n",
      "\n",
      "Global Y-max Storage Modulus = 2769.0 Pa\n",
      "Global Y-max Loss Modulus = 398.8 Pa\n",
      "\n",
      "Detected Focus/Distance naming convention\n",
      "Focus files: 54, Distance files: 54\n",
      "Creating separate plots for Focus and Distance measurements\n",
      "\n",
      "\n",
      "============================================================\n",
      "CREATING FOCUS PLOTS\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ“ Saved statistical results to: C:\\Users\\INSTRU~1\\ONEDRI~2\\Courses\\YEAR1~1\\FALLSE~1\\CHEN58~1\\HYDROG~1\\RHEOME~1\\FALLSE~1\\plots_by_gelatin_focus\\all_t_test_results.txt\n",
      "\n",
      "Created 19 Focus plots in: plots_by_gelatin_focus\n",
      "\n",
      "============================================================\n",
      "CREATING DISTANCE PLOTS (with dot pattern)\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ“ Saved statistical results to: C:\\Users\\INSTRU~1\\ONEDRI~2\\Courses\\YEAR1~1\\FALLSE~1\\CHEN58~1\\HYDROG~1\\RHEOME~1\\FALLSE~1\\plots_by_gelatin_distance\\all_t_test_results.txt\n",
      "\n",
      "Created 19 Distance plots with dot pattern in: plots_by_gelatin_distance\n",
      "\n",
      "============================================================\n",
      "CREATING FOCUS VS DISTANCE COMPARISON CHARTS\n",
      "============================================================\n",
      "\n",
      "\n",
      "Creating Focus vs Distance comparison charts...\n",
      "Output directory: focus_vs_distance\n",
      "\n",
      "  Created: focus_vs_distance_2%_DBPC_HMSN_storage.png\n",
      "  Created: focus_vs_distance_2%_DBPC_HMSN_loss.png\n",
      "  Created: focus_vs_distance_2%_MSN_storage.png\n",
      "  Created: focus_vs_distance_2%_MSN_loss.png\n",
      "  Created: focus_vs_distance_2%_1X_PBS_storage.png\n",
      "  Created: focus_vs_distance_2%_1X_PBS_loss.png\n",
      "  Created: focus_vs_distance_4%_DBPC_HMSN_storage.png\n",
      "  Created: focus_vs_distance_4%_DBPC_HMSN_loss.png\n",
      "  Created: focus_vs_distance_4%_MSN_storage.png\n",
      "  Created: focus_vs_distance_4%_MSN_loss.png\n",
      "  Created: focus_vs_distance_4%_1X_PBS_storage.png\n",
      "  Created: focus_vs_distance_4%_1X_PBS_loss.png\n",
      "  Created: focus_vs_distance_8%_DBPC_HMSN_storage.png\n",
      "  Created: focus_vs_distance_8%_DBPC_HMSN_loss.png\n",
      "  Created: focus_vs_distance_8%_MSN_storage.png\n",
      "  Created: focus_vs_distance_8%_MSN_loss.png\n",
      "  Created: focus_vs_distance_8%_1X_PBS_storage.png\n",
      "  Created: focus_vs_distance_8%_1X_PBS_loss.png\n",
      "\n",
      "Saved paired t-test results to: focus_vs_distance_paired_t_tests.txt\n",
      "\n",
      "Completed Focus vs Distance comparison charts\n",
      "Saved to: focus_vs_distance\n",
      "\n",
      "============================================================\n",
      "BATCH PROCESSING COMPLETE!\n",
      "Total CSV files processed: 108\n",
      "Focus vs Distance comparison plots created: 19\n",
      "============================================================\n",
      "\n",
      "âœ“ Check the output folders for plots and statistical results!\n",
      "âœ“ Statistical test results saved in text files\n",
      "\n",
      "============================================================\n",
      "OUTPUT FOLDER STRUCTURE:\n",
      "============================================================\n",
      "ðŸ“ C:\\Users\\INSTRU~1\\ONEDRI~2\\Courses\\YEAR1~1\\FALLSE~1\\CHEN58~1\\HYDROG~1\\RHEOME~1\\FALLSE~1/\n",
      "   â”œâ”€â”€ ðŸ“ plots_by_gelatin_focus/\n",
      "   â”œâ”€â”€ ðŸ“ plots_by_gelatin_distance/\n",
      "   â””â”€â”€ ðŸ“ focus_vs_distance/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === Block 3: Plot All CSV Files Organized by Gelatin Concentration + Focus/Distance ===\n",
    "# Small-panel (3.77\" x 2.50\") figures with legible fonts, 600 dpi\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "from matplotlib.legend_handler import HandlerBase\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import FuncFormatter, AutoMinorLocator\n",
    "import matplotlib as mpl\n",
    "import colorsys\n",
    "from scipy.stats import ttest_ind, ttest_rel\n",
    "from matplotlib.colors import to_rgb, to_hex\n",
    "import re\n",
    "\n",
    "# ===== Small-panel & style settings =====\n",
    "PANEL_W_IN, PANEL_H_IN = 3.77, 2.50\n",
    "EXPORT_DPI = 600\n",
    "\n",
    "AXIS_FS = 11\n",
    "TICK_FS = 10\n",
    "LEGEND_FS = 9\n",
    "\n",
    "ERR_KW = dict(elinewidth=0.6, capsize=3, capthick=0.6, ecolor=\"black\")\n",
    "\n",
    "HATCH_LW_BARS = 0.4\n",
    "HATCH_LW_LEG  = 0.6\n",
    "mpl.rcParams['hatch.linewidth'] = HATCH_LW_BARS\n",
    "\n",
    "# ===== Pattern definitions (shared across functions) =====\n",
    "GELATIN_PATTERNS = {\n",
    "    '2%': '',\n",
    "    '4%': '//',\n",
    "    '8%': 'xx'\n",
    "}\n",
    "\n",
    "class HandlerLineWithShade(HandlerBase):\n",
    "    def create_artists(self, legend, orig_handle,\n",
    "                      xdescent, ydescent, width, height, fontsize,\n",
    "                      trans):\n",
    "        color = orig_handle.get_color()\n",
    "        rect = Rectangle((xdescent, ydescent),\n",
    "                         width, height,\n",
    "                         facecolor=color,\n",
    "                         edgecolor='none',\n",
    "                         alpha=0.2,\n",
    "                         transform=trans)\n",
    "        line = Line2D([xdescent, xdescent + width],\n",
    "                      [ydescent + height/2, ydescent + height/2],\n",
    "                      color=color,\n",
    "                      linewidth=3,\n",
    "                      transform=trans)\n",
    "        return [rect, line]\n",
    "\n",
    "def small_panel_ax():\n",
    "    fig, ax = plt.subplots(figsize=(PANEL_W_IN, PANEL_H_IN))\n",
    "    ax.tick_params(axis='both', which='major',\n",
    "                   labelsize=TICK_FS, length=3, width=0.8, direction='out')\n",
    "    ax.tick_params(axis='y', which='minor',\n",
    "                   length=2, width=0.5, direction='out')\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_linewidth(0.8)\n",
    "    return fig, ax\n",
    "\n",
    "def small_legend(ax, *args, **kwargs):\n",
    "    leg = ax.legend(*args, fontsize=LEGEND_FS, title_fontsize=LEGEND_FS,\n",
    "                    frameon=True, fancybox=True, framealpha=1.0, **kwargs)\n",
    "    fr = leg.get_frame()\n",
    "    fr.set_edgecolor(\"#D0D0D0\")\n",
    "    fr.set_linewidth(0.8)\n",
    "    return leg\n",
    "\n",
    "def adjust_lightness(hex_color, factor):\n",
    "    \"\"\"Lighten (factor > 1) or darken (factor < 1) a hex color.\"\"\"\n",
    "    r, g, b = to_rgb(hex_color)\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "    l = max(0, min(1, l * factor))\n",
    "    r2, g2, b2 = colorsys.hls_to_rgb(h, l, s)\n",
    "    return to_hex((r2, g2, b2))\n",
    "\n",
    "def extract_gelatin_concentration(file_path):\n",
    "    \"\"\"Extract gelatin concentration from file path.\"\"\"\n",
    "    match = re.search(r'(\\d+)%\\s*gelatin', file_path, re.IGNORECASE)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}%\"\n",
    "    return None\n",
    "\n",
    "def detect_cycle_in_filename(filename):\n",
    "    \"\"\"Detect which cycle type is in the filename.\"\"\"\n",
    "    lower = filename.lower()\n",
    "    if '40 cycles' in lower or '40cycles' in lower:\n",
    "        return '40 cycles', '3.6%'\n",
    "    elif '28 cycles' in lower or '28cycles' in lower:\n",
    "        return '28 cycles', '2.5%'\n",
    "    elif '0 cycles' in lower or '0cycles' in lower:\n",
    "        return '0 cycles', '0%'\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def detect_focus_distance_filename(filename):\n",
    "    \"\"\"Detect whether file is Focus (F) or Distance (D) measurement.\"\"\"\n",
    "    basename = os.path.basename(filename).upper()\n",
    "    if basename.startswith('F_'):\n",
    "        return 'Focus'\n",
    "    elif basename.startswith('D_'):\n",
    "        return 'Distance'\n",
    "    return None\n",
    "\n",
    "def find_common_base_directory(csv_files):\n",
    "    \"\"\"Find the common parent directory for all CSV files.\"\"\"\n",
    "    if not csv_files:\n",
    "        return os.getcwd()\n",
    "    \n",
    "    # Get all directory paths\n",
    "    dirs = [os.path.dirname(os.path.abspath(f)) for f in csv_files]\n",
    "    \n",
    "    # Find common path\n",
    "    common = os.path.commonpath(dirs)\n",
    "    \n",
    "    return common\n",
    "\n",
    "def plot_gelatin_organized_data(csv_files, global_ymax_storage, global_ymax_loss, base_directory, add_dots=False, output_suffix=\"\"):\n",
    "    \"\"\"Plot data organized by gelatin concentration for both storage and loss modulus.\"\"\"\n",
    "    \n",
    "    # --- Settings ---\n",
    "    start_row = 0\n",
    "    freq_window = (0.3, 1.0)\n",
    "    \n",
    "    cycle_map = {\n",
    "        '0 cycles': '0%',\n",
    "        '28 cycles': '2.5%',\n",
    "        '40 cycles': '3.6%'\n",
    "    }\n",
    "    \n",
    "    particle_colors = {\n",
    "        'DBPC HMSN': '#D55E00',\n",
    "        'MSN': '#009E73',\n",
    "        '1X PBS': '#0072B2',\n",
    "    }\n",
    "    \n",
    "    shade_map = {\n",
    "        '0%': 1.3,\n",
    "        '2.5%': 1.0,\n",
    "        '3.6%': 0.7\n",
    "    }\n",
    "    \n",
    "    # Add dots to patterns if this is for Distance plots\n",
    "    gelatin_patterns = {k: v + ('...' if add_dots else '') for k, v in GELATIN_PATTERNS.items()}\n",
    "\n",
    "    # Collect all data\n",
    "    all_data = {}\n",
    "    file_data = {}\n",
    "    \n",
    "    for csv_path in csv_files:\n",
    "        gelatin_conc = extract_gelatin_concentration(csv_path)\n",
    "        if not gelatin_conc:\n",
    "            continue\n",
    "            \n",
    "        if gelatin_conc not in all_data:\n",
    "            all_data[gelatin_conc] = {\n",
    "                p: {dc: {'storage': [], 'loss': []} for dc in cycle_map.values()} \n",
    "                for p in particle_colors\n",
    "            }\n",
    "            file_data[gelatin_conc] = {\n",
    "                p: {dc: {'Focus': [], 'Distance': []} for dc in cycle_map.values()}\n",
    "                for p in particle_colors\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            filename = os.path.basename(csv_path)\n",
    "            lower = filename.lower()\n",
    "            \n",
    "            cycle_type, dc = detect_cycle_in_filename(filename)\n",
    "            if not (cycle_type and dc):\n",
    "                continue\n",
    "            \n",
    "            if 'dbpc hmsn' in lower or 'dbpc_hmsn' in lower or 'dbpchmsn' in lower:\n",
    "                particle = 'DBPC HMSN'\n",
    "            elif 'msn' in lower and 'hmsn' not in lower:\n",
    "                particle = 'MSN'\n",
    "            elif '1x pbs' in lower or '1xpbs' in lower or 'pbs' in lower:\n",
    "                particle = '1X PBS'\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            focus_dist = detect_focus_distance_filename(filename)\n",
    "            if focus_dist:\n",
    "                file_data[gelatin_conc][particle][dc][focus_dist].append(csv_path)\n",
    "            \n",
    "            df = pd.read_csv(csv_path, skiprows=53, header=None)\n",
    "            \n",
    "            storage_data = df.iloc[start_row:, [0, 6]].dropna()\n",
    "            storage_data.columns = ['Angular Frequency', 'Storage Modulus']\n",
    "            storage_data = storage_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "            \n",
    "            loss_data = df.iloc[start_row:, [0, 7]].dropna()\n",
    "            loss_data.columns = ['Angular Frequency', 'Loss Modulus']\n",
    "            loss_data = loss_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "            \n",
    "            if not storage_data.empty:\n",
    "                all_data[gelatin_conc][particle][dc]['storage'].append(storage_data)\n",
    "            if not loss_data.empty:\n",
    "                all_data[gelatin_conc][particle][dc]['loss'].append(loss_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    output_dir = os.path.join(base_directory, f\"plots_by_gelatin{output_suffix}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    plots_created = []\n",
    "    t_test_results = []\n",
    "    paired_t_test_results = []\n",
    "    \n",
    "    sorted_gelatin = sorted(all_data.keys(), key=lambda x: int(x.rstrip('%')))\n",
    "    \n",
    "    # Bar plots for each gelatin concentration\n",
    "    for gelatin_conc in sorted_gelatin:\n",
    "        gelatin_data = all_data[gelatin_conc]\n",
    "        \n",
    "        for modulus_type in ['storage', 'loss']:\n",
    "            modulus_label = 'Storage' if modulus_type == 'storage' else 'Loss'\n",
    "            modulus_symbol = \"G'\" if modulus_type == 'storage' else 'G\"'\n",
    "            global_ymax = global_ymax_storage if modulus_type == 'storage' else global_ymax_loss\n",
    "            \n",
    "            particles_with_data = [p for p in particle_colors \n",
    "                                  if any(gelatin_data[p][dc][modulus_type] for dc in cycle_map.values())]\n",
    "            \n",
    "            if not particles_with_data:\n",
    "                continue\n",
    "            \n",
    "            fig, ax = small_panel_ax()\n",
    "            \n",
    "            sorted_dcs = sorted([dc for dc in cycle_map.values() if \n",
    "                               any(gelatin_data[p][dc][modulus_type] for p in particles_with_data)],\n",
    "                               key=lambda x: float(x.rstrip('%')))\n",
    "            \n",
    "            x = np.arange(len(particles_with_data))\n",
    "            width = 0.6\n",
    "            offsets = np.linspace(-width, width, len(sorted_dcs))\n",
    "            \n",
    "            for i, dc in enumerate(sorted_dcs):\n",
    "                means = []\n",
    "                stds = []\n",
    "                \n",
    "                for particle in particles_with_data:\n",
    "                    dfs = gelatin_data[particle][dc][modulus_type]\n",
    "                    if dfs:\n",
    "                        vals = []\n",
    "                        for df in dfs:\n",
    "                            mask = (df['Angular Frequency'] >= freq_window[0]) & (df['Angular Frequency'] <= freq_window[1])\n",
    "                            mean_val = df.loc[mask, f'{modulus_label} Modulus'].mean()\n",
    "                            if not np.isnan(mean_val):\n",
    "                                vals.append(mean_val)\n",
    "                        \n",
    "                        means.append(np.mean(vals) if vals else 0)\n",
    "                        stds.append(np.std(vals) if vals else 0)\n",
    "                    else:\n",
    "                        means.append(0)\n",
    "                        stds.append(0)\n",
    "                \n",
    "                bars = ax.bar(x + offsets[i], means, width, yerr=stds, \n",
    "                             label=dc, error_kw=ERR_KW)\n",
    "                \n",
    "                for j, bar in enumerate(bars):\n",
    "                    particle = particles_with_data[j]\n",
    "                    base_color = particle_colors[particle]\n",
    "                    bar_color = adjust_lightness(base_color, shade_map[dc])\n",
    "                    bar.set_color(bar_color)\n",
    "                    bar.set_hatch(gelatin_patterns[gelatin_conc])\n",
    "                    bar.set_edgecolor('gray')\n",
    "                    bar.set_linewidth(0.5)\n",
    "            \n",
    "            # Independent t-tests between duty cycles\n",
    "            for p_idx, particle in enumerate(particles_with_data):\n",
    "                for i in range(len(sorted_dcs) - 1):\n",
    "                    dc1, dc2 = sorted_dcs[i], sorted_dcs[i+1]\n",
    "                    \n",
    "                    vals1 = []\n",
    "                    for df in gelatin_data[particle][dc1][modulus_type]:\n",
    "                        mask = (df['Angular Frequency'] >= freq_window[0]) & (df['Angular Frequency'] <= freq_window[1])\n",
    "                        mean_val = df.loc[mask, f'{modulus_label} Modulus'].mean()\n",
    "                        if not np.isnan(mean_val):\n",
    "                            vals1.append(mean_val)\n",
    "                    \n",
    "                    vals2 = []\n",
    "                    for df in gelatin_data[particle][dc2][modulus_type]:\n",
    "                        mask = (df['Angular Frequency'] >= freq_window[0]) & (df['Angular Frequency'] <= freq_window[1])\n",
    "                        mean_val = df.loc[mask, f'{modulus_label} Modulus'].mean()\n",
    "                        if not np.isnan(mean_val):\n",
    "                            vals2.append(mean_val)\n",
    "                    \n",
    "                    if len(vals1) >= 2 and len(vals2) >= 2:\n",
    "                        t_stat, p_val_two = ttest_ind(vals1, vals2)\n",
    "                        p_val_one = p_val_two / 2\n",
    "                        \n",
    "                        sig_two = \"***\" if p_val_two < 0.001 else \"**\" if p_val_two < 0.01 else \"*\" if p_val_two < 0.05 else \"ns\"\n",
    "                        sig_one = \"***\" if p_val_one < 0.001 else \"**\" if p_val_one < 0.01 else \"*\" if p_val_one < 0.05 else \"ns\"\n",
    "                        \n",
    "                        result_str = (f\"{gelatin_conc} Gelatin - {particle} {modulus_label}: \"\n",
    "                                    f\"{dc1} vs {dc2} | \"\n",
    "                                    f\"Two-tailed: p={p_val_two:.4f} {sig_two}, \"\n",
    "                                    f\"One-tailed: p={p_val_one:.4f} {sig_one}\")\n",
    "                        t_test_results.append(result_str)\n",
    "            \n",
    "            ax.set_xlabel('Particle Type', fontsize=AXIS_FS)\n",
    "            ax.set_ylabel(f'{modulus_label} Modulus ({modulus_symbol})\\n(Pa)', fontsize=AXIS_FS)\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(particles_with_data, fontsize=TICK_FS)\n",
    "            ax.set_ylim(0, global_ymax * 1.1)\n",
    "            ax.ticklabel_format(style='plain', axis='y', useOffset=False)\n",
    "            ax.yaxis.set_major_formatter(FuncFormatter(lambda x, p: f'{int(x):,}'))\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            gray_map = {'0%': '#E0E0E0', '2.5%': '#A0A0A0', '3.6%': '#606060'}\n",
    "            \n",
    "            with mpl.rc_context({'hatch.linewidth': HATCH_LW_LEG}):\n",
    "                handles = [Patch(facecolor=gray_map.get(dc, 'gray'), \n",
    "                                 edgecolor='gray', \n",
    "                                 linewidth=0.5,\n",
    "                                 alpha=0.8 if dc == '2.5%' else 1.0,\n",
    "                                 hatch=gelatin_patterns[gelatin_conc])\n",
    "                           for dc in sorted_dcs]\n",
    "            small_legend(ax, handles=handles,\n",
    "                         labels=sorted_dcs,\n",
    "                         title=\"Duty Cycle\", \n",
    "                         bbox_to_anchor=(1.00, 1), \n",
    "                         loc='upper left')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            filename = f\"gelatin_{gelatin_conc}_{modulus_type}_comparison.png\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            plt.savefig(filepath, dpi=EXPORT_DPI, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            plots_created.append(filename)\n",
    "    \n",
    "    # Ratio bar plots\n",
    "    for gelatin_conc in sorted_gelatin:\n",
    "        gelatin_data = all_data[gelatin_conc]\n",
    "        \n",
    "        particles_with_data = [p for p in particle_colors \n",
    "                              if any(gelatin_data[p][dc]['storage'] and gelatin_data[p][dc]['loss'] \n",
    "                                    for dc in cycle_map.values())]\n",
    "        \n",
    "        if not particles_with_data:\n",
    "            continue\n",
    "        \n",
    "        fig, ax = small_panel_ax()\n",
    "        \n",
    "        sorted_dcs = sorted([dc for dc in cycle_map.values() if \n",
    "                           any(gelatin_data[p][dc]['storage'] and gelatin_data[p][dc]['loss'] \n",
    "                              for p in particles_with_data)],\n",
    "                           key=lambda x: float(x.rstrip('%')))\n",
    "        \n",
    "        x = np.arange(len(particles_with_data))\n",
    "        width = 0.6\n",
    "        offsets = np.linspace(-width, width, len(sorted_dcs))\n",
    "        \n",
    "        for i, dc in enumerate(sorted_dcs):\n",
    "            means = []\n",
    "            stds = []\n",
    "            \n",
    "            for particle in particles_with_data:\n",
    "                storage_list = gelatin_data[particle][dc]['storage']\n",
    "                loss_list = gelatin_data[particle][dc]['loss']\n",
    "                \n",
    "                if storage_list and loss_list:\n",
    "                    ratios = []\n",
    "                    for s_df, l_df in zip(storage_list, loss_list):\n",
    "                        merged = pd.merge(s_df, l_df, on='Angular Frequency', suffixes=('_s', '_l'))\n",
    "                        mask = (merged['Angular Frequency'] >= freq_window[0]) & (merged['Angular Frequency'] <= freq_window[1])\n",
    "                        if mask.any():\n",
    "                            ratio = (merged.loc[mask, 'Storage Modulus'].mean() / \n",
    "                                   merged.loc[mask, 'Loss Modulus'].mean())\n",
    "                            if not np.isnan(ratio):\n",
    "                                ratios.append(ratio)\n",
    "                    \n",
    "                    means.append(np.mean(ratios) if ratios else 0)\n",
    "                    stds.append(np.std(ratios) if ratios else 0)\n",
    "                else:\n",
    "                    means.append(0)\n",
    "                    stds.append(0)\n",
    "            \n",
    "            bars = ax.bar(x + offsets[i], means, width, yerr=stds,\n",
    "                         label=dc, error_kw=ERR_KW)\n",
    "            \n",
    "            for j, bar in enumerate(bars):\n",
    "                particle = particles_with_data[j]\n",
    "                base_color = particle_colors[particle]\n",
    "                bar_color = adjust_lightness(base_color, shade_map[dc])\n",
    "                bar.set_color(bar_color)\n",
    "                bar.set_hatch(gelatin_patterns[gelatin_conc])\n",
    "                bar.set_edgecolor('gray')\n",
    "                bar.set_linewidth(0.5)\n",
    "        \n",
    "        ax.set_xlabel('Particle Type', fontsize=AXIS_FS)\n",
    "        ax.set_ylabel(\"G'/G'' Ratio\", fontsize=AXIS_FS)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(particles_with_data, fontsize=TICK_FS)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        \n",
    "        gray_map = {'0%': '#E0E0E0', '2.5%': '#A0A0A0', '3.6%': '#606060'}\n",
    "        \n",
    "        with mpl.rc_context({'hatch.linewidth': HATCH_LW_LEG}):\n",
    "            handles = [Patch(facecolor=gray_map.get(dc, 'gray'), \n",
    "                             edgecolor='gray', \n",
    "                             linewidth=0.5,\n",
    "                             alpha=0.8 if dc == '2.5%' else 1.0,\n",
    "                             hatch=gelatin_patterns.get(gelatin_conc, ''))\n",
    "                       for dc in sorted_dcs]\n",
    "        small_legend(ax, handles=handles,\n",
    "                     labels=sorted_dcs,\n",
    "                     title=\"Duty Cycle\", \n",
    "                     bbox_to_anchor=(1.00, 1), \n",
    "                     loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"gelatin_{gelatin_conc}_ratio_comparison.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=EXPORT_DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plots_created.append(filename)\n",
    "    \n",
    "    # Ratio line plots\n",
    "    for gelatin_conc in sorted_gelatin:\n",
    "        gelatin_data = all_data[gelatin_conc]\n",
    "        \n",
    "        for particle in particle_colors:\n",
    "            has_data = any(gelatin_data[particle][dc]['storage'] and \n",
    "                          gelatin_data[particle][dc]['loss'] for dc in cycle_map.values())\n",
    "            if not has_data:\n",
    "                continue\n",
    "            \n",
    "            base_color = particle_colors[particle]\n",
    "            fig, ax = small_panel_ax()\n",
    "            \n",
    "            legend_handles = []\n",
    "            legend_labels = []\n",
    "            \n",
    "            sorted_dcs = sorted([dc for dc in cycle_map.values() if \n",
    "                               gelatin_data[particle][dc]['storage'] and \n",
    "                               gelatin_data[particle][dc]['loss']], \n",
    "                               key=lambda x: float(x.rstrip('%')))\n",
    "            \n",
    "            for dc in sorted_dcs:\n",
    "                storage_list = gelatin_data[particle][dc]['storage']\n",
    "                loss_list = gelatin_data[particle][dc]['loss']\n",
    "                if not storage_list or not loss_list:\n",
    "                    continue\n",
    "                    \n",
    "                color = adjust_lightness(base_color, shade_map[dc])\n",
    "                \n",
    "                all_ratios = []\n",
    "                for s_df, l_df in zip(storage_list, loss_list):\n",
    "                    merged = pd.merge(s_df, l_df, on='Angular Frequency', suffixes=('_s', '_l'))\n",
    "                    merged['Ratio'] = merged['Storage Modulus'] / merged['Loss Modulus']\n",
    "                    all_ratios.append(merged[['Angular Frequency', 'Ratio']])\n",
    "                \n",
    "                combined = pd.concat(all_ratios)\n",
    "                g = combined.groupby('Angular Frequency')['Ratio']\n",
    "                mean_r, std_r = g.mean(), g.std()\n",
    "                \n",
    "                ax.plot(mean_r.index, mean_r.values, color=color, lw=3, label=dc)\n",
    "                ax.fill_between(mean_r.index, mean_r - std_r, mean_r + std_r, color=color, alpha=0.2)\n",
    "                \n",
    "                legend_handles.append(color)\n",
    "                legend_labels.append(dc)\n",
    "            \n",
    "            ax.set_xscale('log')\n",
    "            ax.set_yscale('log')\n",
    "            ax.set_ylim(1, 1000)\n",
    "            ax.set_xlabel(\"Angular Frequency (rad/s)\", fontsize=AXIS_FS)\n",
    "            ax.set_ylabel(\"G'/G'' Ratio\", fontsize=AXIS_FS)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=TICK_FS, length=3, width=0.8)\n",
    "            \n",
    "            custom_lines = [Line2D([0], [0], color=color, lw=3) for color in legend_handles]\n",
    "            small_legend(ax, custom_lines, legend_labels, title=\"Duty Cycle\", \n",
    "                         bbox_to_anchor=(1, 1.02), loc='upper left',\n",
    "                         handler_map={Line2D: HandlerLineWithShade()})\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            filename = f\"gelatin_{gelatin_conc}_{particle}_ratio_line.png\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            plt.savefig(filepath, dpi=EXPORT_DPI, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            plots_created.append(filename)\n",
    "    \n",
    "    # Save t-test results\n",
    "    if t_test_results:\n",
    "        t_test_file = os.path.join(output_dir, \"all_t_test_results.txt\")\n",
    "        with open(t_test_file, 'w') as f:\n",
    "            f.write(\"Statistical Analysis Results\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\")\n",
    "            f.write(\"Note: Two-tailed tests detect any difference.\\n\")\n",
    "            f.write(\"      One-tailed p-value = Two-tailed p-value / 2\\n\")\n",
    "            f.write(\"      (assumes storage modulus decreases with duty cycle)\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"INDEPENDENT T-TESTS (Duty Cycle Comparisons)\\n\")\n",
    "            f.write(\"-\"*60 + \"\\n\\n\")\n",
    "            for gelatin_conc in sorted_gelatin:\n",
    "                gelatin_tests = [r for r in t_test_results if r.startswith(f\"{gelatin_conc} Gelatin\")]\n",
    "                if gelatin_tests:\n",
    "                    f.write(f\"{gelatin_conc} Gelatin:\\n\")\n",
    "                    f.write(\"-\"*30 + \"\\n\")\n",
    "                    for result in gelatin_tests:\n",
    "                        f.write(result.replace(f\"{gelatin_conc} Gelatin - \", \"  \") + \"\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "        \n",
    "        plots_created.append(\"all_t_test_results.txt\")\n",
    "        print(f\"\\nâœ“ Saved statistical results to: {t_test_file}\")\n",
    "    \n",
    "    return output_dir, plots_created, file_data\n",
    "\n",
    "\n",
    "def create_focus_vs_distance_comparisons(csv_files, global_ymax_storage, global_ymax_loss, base_directory):\n",
    "    \"\"\"Create bar charts comparing Focus vs Distance measurements for each gelatin/particle/duty cycle.\"\"\"\n",
    "    \n",
    "    focus_vs_distance_dir = os.path.join(base_directory, \"focus_vs_distance\")\n",
    "    os.makedirs(focus_vs_distance_dir, exist_ok=True)\n",
    "    \n",
    "    freq_window = (0.3, 1.0)\n",
    "    \n",
    "    cycle_map = {\n",
    "        '0 cycles': '0%',\n",
    "        '28 cycles': '2.5%',\n",
    "        '40 cycles': '3.6%'\n",
    "    }\n",
    "    \n",
    "    particle_colors = {\n",
    "        'DBPC HMSN': '#D55E00',\n",
    "        'MSN': '#009E73',\n",
    "        '1X PBS': '#0072B2',\n",
    "    }\n",
    "    \n",
    "    shade_map = {\n",
    "        '0%': 1.3,\n",
    "        '2.5%': 1.0,\n",
    "        '3.6%': 0.7\n",
    "    }\n",
    "    \n",
    "    # Organize files by gelatin, particle, duty cycle, and Focus/Distance\n",
    "    file_data = {}\n",
    "    \n",
    "    for csv_path in csv_files:\n",
    "        gelatin_conc = extract_gelatin_concentration(csv_path)\n",
    "        if not gelatin_conc:\n",
    "            continue\n",
    "        \n",
    "        filename = os.path.basename(csv_path)\n",
    "        lower = filename.lower()\n",
    "        \n",
    "        cycle_type, dc = detect_cycle_in_filename(filename)\n",
    "        if not (cycle_type and dc):\n",
    "            continue\n",
    "        \n",
    "        if 'dbpc hmsn' in lower or 'dbpc_hmsn' in lower or 'dbpchmsn' in lower:\n",
    "            particle = 'DBPC HMSN'\n",
    "        elif 'msn' in lower and 'hmsn' not in lower:\n",
    "            particle = 'MSN'\n",
    "        elif '1x pbs' in lower or '1xpbs' in lower or 'pbs' in lower:\n",
    "            particle = '1X PBS'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        focus_dist = detect_focus_distance_filename(filename)\n",
    "        if not focus_dist:\n",
    "            continue\n",
    "        \n",
    "        if gelatin_conc not in file_data:\n",
    "            file_data[gelatin_conc] = {\n",
    "                p: {dc: {'Focus': [], 'Distance': []} for dc in cycle_map.values()}\n",
    "                for p in particle_colors\n",
    "            }\n",
    "        \n",
    "        file_data[gelatin_conc][particle][dc][focus_dist].append(csv_path)\n",
    "    \n",
    "    sorted_gelatin = sorted(file_data.keys(), key=lambda x: int(x.rstrip('%')))\n",
    "    plots_created = []\n",
    "    paired_t_test_results = []\n",
    "    \n",
    "    print(f\"\\nCreating Focus vs Distance comparison charts...\")\n",
    "    print(f\"Output directory: {os.path.relpath(focus_vs_distance_dir, base_directory)}\\n\")\n",
    "    \n",
    "    # Create comparison plots for each combination\n",
    "    for gelatin_conc in sorted_gelatin:\n",
    "        focus_pattern = GELATIN_PATTERNS[gelatin_conc]\n",
    "        distance_pattern = GELATIN_PATTERNS[gelatin_conc] + '...'\n",
    "        \n",
    "        for particle in particle_colors:\n",
    "            for modulus_type in ['storage', 'loss']:\n",
    "                modulus_label = 'Storage' if modulus_type == 'storage' else 'Loss'\n",
    "                modulus_symbol = \"G'\" if modulus_type == 'storage' else 'G\"'\n",
    "                global_ymax = global_ymax_storage if modulus_type == 'storage' else global_ymax_loss\n",
    "                col_index = 6 if modulus_type == 'storage' else 7\n",
    "                \n",
    "                # Check which duty cycles have both Focus and Distance data\n",
    "                available_dcs = []\n",
    "                dc_focus_means = {}\n",
    "                dc_focus_stds = {}\n",
    "                dc_distance_means = {}\n",
    "                dc_distance_stds = {}\n",
    "                \n",
    "                for dc in sorted(cycle_map.values(), key=lambda x: float(x.rstrip('%'))):\n",
    "                    focus_files_dc = file_data[gelatin_conc][particle][dc]['Focus']\n",
    "                    distance_files_dc = file_data[gelatin_conc][particle][dc]['Distance']\n",
    "                    \n",
    "                    if focus_files_dc and distance_files_dc:\n",
    "                        available_dcs.append(dc)\n",
    "                        \n",
    "                        # Process Focus files\n",
    "                        if focus_files_dc:\n",
    "                            focus_vals = []\n",
    "                            for f_path in focus_files_dc:\n",
    "                                try:\n",
    "                                    df = pd.read_csv(f_path, skiprows=53, header=None)\n",
    "                                    data = df.iloc[:, [0, col_index]].dropna()\n",
    "                                    data.columns = ['Angular Frequency', f'{modulus_label} Modulus']\n",
    "                                    data = data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "                                    \n",
    "                                    mean_val = data[(data['Angular Frequency']>=freq_window[0]) &\n",
    "                                                   (data['Angular Frequency']<=freq_window[1])][f'{modulus_label} Modulus'].mean()\n",
    "                                    if not np.isnan(mean_val):\n",
    "                                        focus_vals.append(mean_val)\n",
    "                                except:\n",
    "                                    pass\n",
    "                            \n",
    "                            dc_focus_means[dc] = np.mean(focus_vals) if focus_vals else 0\n",
    "                            dc_focus_stds[dc] = np.std(focus_vals) if focus_vals else 0\n",
    "                        else:\n",
    "                            dc_focus_means[dc] = 0\n",
    "                            dc_focus_stds[dc] = 0\n",
    "                        \n",
    "                        # Process Distance files\n",
    "                        if distance_files_dc:\n",
    "                            distance_vals = []\n",
    "                            for d_path in distance_files_dc:\n",
    "                                try:\n",
    "                                    df = pd.read_csv(d_path, skiprows=53, header=None)\n",
    "                                    data = df.iloc[:, [0, col_index]].dropna()\n",
    "                                    data.columns = ['Angular Frequency', f'{modulus_label} Modulus']\n",
    "                                    data = data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "                                    \n",
    "                                    mean_val = data[(data['Angular Frequency']>=freq_window[0]) &\n",
    "                                                   (data['Angular Frequency']<=freq_window[1])][f'{modulus_label} Modulus'].mean()\n",
    "                                    if not np.isnan(mean_val):\n",
    "                                        distance_vals.append(mean_val)\n",
    "                                except:\n",
    "                                    pass\n",
    "                            \n",
    "                            dc_distance_means[dc] = np.mean(distance_vals) if distance_vals else 0\n",
    "                            dc_distance_stds[dc] = np.std(distance_vals) if distance_vals else 0\n",
    "                        else:\n",
    "                            dc_distance_means[dc] = 0\n",
    "                            dc_distance_stds[dc] = 0\n",
    "                        \n",
    "                        # Paired t-test between Focus and Distance\n",
    "                        if focus_vals and distance_vals and len(focus_vals) == len(distance_vals) and len(focus_vals) >= 2:\n",
    "                            try:\n",
    "                                t_stat, p_val_two = ttest_rel(focus_vals, distance_vals)\n",
    "                                p_val_one = p_val_two / 2\n",
    "                                \n",
    "                                sig_two = \"***\" if p_val_two < 0.001 else \"**\" if p_val_two < 0.01 else \"*\" if p_val_two < 0.05 else \"ns\"\n",
    "                                sig_one = \"***\" if p_val_one < 0.001 else \"**\" if p_val_one < 0.01 else \"*\" if p_val_one < 0.05 else \"ns\"\n",
    "                                \n",
    "                                result_str = (f\"{gelatin_conc} Gelatin - {particle} {modulus_label} @ {dc}: \"\n",
    "                                            f\"Focus vs Distance | \"\n",
    "                                            f\"Two-tailed: p={p_val_two:.4f} {sig_two}, \"\n",
    "                                            f\"One-tailed: p={p_val_one:.4f} {sig_one} | \"\n",
    "                                            f\"n={len(focus_vals)}\")\n",
    "                                paired_t_test_results.append(result_str)\n",
    "                            except:\n",
    "                                pass\n",
    "                \n",
    "                if not available_dcs:\n",
    "                    continue\n",
    "                \n",
    "                # Create bar chart\n",
    "                fig, ax = small_panel_ax()\n",
    "                \n",
    "                x = np.arange(len(available_dcs))\n",
    "                width = 0.35\n",
    "                \n",
    "                base_color = particle_colors[particle]\n",
    "                \n",
    "                # Focus bars (lighter shade)\n",
    "                focus_means = [dc_focus_means[dc] for dc in available_dcs]\n",
    "                focus_stds = [dc_focus_stds[dc] for dc in available_dcs]\n",
    "                focus_bars = ax.bar(x - width/2, focus_means, width, \n",
    "                                   yerr=focus_stds, label='Focus', error_kw=ERR_KW)\n",
    "                \n",
    "                # Distance bars (darker shade)\n",
    "                distance_means = [dc_distance_means[dc] for dc in available_dcs]\n",
    "                distance_stds = [dc_distance_stds[dc] for dc in available_dcs]\n",
    "                distance_bars = ax.bar(x + width/2, distance_means, width,\n",
    "                                      yerr=distance_stds, label='Distance', error_kw=ERR_KW)\n",
    "                \n",
    "                # Color and pattern bars using shade_map for each duty cycle\n",
    "                for i, dc in enumerate(available_dcs):\n",
    "                    # Get base shade for this duty cycle\n",
    "                    base_shade = shade_map[dc]\n",
    "                    duty_cycle_color = adjust_lightness(base_color, base_shade)\n",
    "                    \n",
    "                    # Focus: gelatin pattern only\n",
    "                    focus_bars[i].set_color(duty_cycle_color)\n",
    "                    focus_bars[i].set_hatch(focus_pattern)\n",
    "                    focus_bars[i].set_edgecolor('gray')\n",
    "                    focus_bars[i].set_linewidth(0.5)\n",
    "                    \n",
    "                    # Distance: gelatin pattern + dots\n",
    "                    distance_bars[i].set_color(duty_cycle_color)\n",
    "                    distance_bars[i].set_hatch(distance_pattern)\n",
    "                    distance_bars[i].set_edgecolor('gray')\n",
    "                    distance_bars[i].set_linewidth(0.5)\n",
    "                \n",
    "                ax.set_xlabel('Duty Cycle (%)', fontsize=AXIS_FS)\n",
    "                ax.set_ylabel(f'{modulus_label} Modulus ({modulus_symbol})\\n(Pa)', fontsize=AXIS_FS)\n",
    "                \n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels([dc.rstrip('%') for dc in available_dcs], fontsize=TICK_FS)\n",
    "                \n",
    "                ax.set_ylim(0, global_ymax * 1.1)\n",
    "                ax.ticklabel_format(style='plain', axis='y', useOffset=False)\n",
    "                ax.yaxis.set_major_formatter(FuncFormatter(lambda x, p: f'{int(x):,}'))\n",
    "                \n",
    "                ax.spines['top'].set_visible(False)\n",
    "                ax.spines['right'].set_visible(False)\n",
    "                \n",
    "                # Legend - use middle duty cycle shade (2.5% = 1.0) for representation\n",
    "                legend_color = adjust_lightness(base_color, 1.0)\n",
    "                with mpl.rc_context({'hatch.linewidth': HATCH_LW_LEG}):\n",
    "                    handles = [\n",
    "                        Patch(facecolor=legend_color, \n",
    "                              edgecolor='gray', linewidth=0.5, hatch=focus_pattern),\n",
    "                        Patch(facecolor=legend_color, \n",
    "                              edgecolor='gray', linewidth=0.5, hatch=distance_pattern)\n",
    "                    ]\n",
    "                small_legend(ax, handles=handles, labels=['Bottom layer (0-4 mm)', 'Top layer (4-8 mm)'],\n",
    "                            title=\"Measurement Type\", \n",
    "                            bbox_to_anchor=(1.00, 1), loc='upper left')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save to subfolder\n",
    "                filename = f\"focus_vs_distance_{gelatin_conc}_{particle.replace(' ', '_')}_{modulus_type}.png\"\n",
    "                filepath = os.path.join(focus_vs_distance_dir, filename)\n",
    "                plt.savefig(filepath, dpi=EXPORT_DPI, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                plots_created.append(filename)\n",
    "                \n",
    "                print(f\"  Created: {filename}\")\n",
    "    \n",
    "    # Save paired t-test results\n",
    "    if paired_t_test_results:\n",
    "        t_test_file = os.path.join(focus_vs_distance_dir, \"focus_vs_distance_paired_t_tests.txt\")\n",
    "        with open(t_test_file, 'w') as f:\n",
    "            f.write(\"PAIRED T-TESTS: Bottom Layer (0-4 mm) vs Top Layer (4-8 mm)\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\")\n",
    "            f.write(\"Note: Two-tailed tests detect any difference.\\n\")\n",
    "            f.write(\"      One-tailed p-value = Two-tailed p-value / 2\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            \n",
    "            for gelatin_conc in sorted_gelatin:\n",
    "                gelatin_tests = [r for r in paired_t_test_results if r.startswith(f\"{gelatin_conc} Gelatin\")]\n",
    "                if gelatin_tests:\n",
    "                    f.write(f\"{gelatin_conc} Gelatin:\\n\")\n",
    "                    f.write(\"-\"*30 + \"\\n\")\n",
    "                    for result in gelatin_tests:\n",
    "                        f.write(result.replace(f\"{gelatin_conc} Gelatin - \", \"  \") + \"\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "        \n",
    "        plots_created.append(\"focus_vs_distance_paired_t_tests.txt\")\n",
    "        print(f\"\\nSaved paired t-test results to: focus_vs_distance_paired_t_tests.txt\")\n",
    "    \n",
    "    print(f\"\\nCompleted Focus vs Distance comparison charts\")\n",
    "    print(f\"Saved to: {os.path.relpath(focus_vs_distance_dir, base_directory)}\")\n",
    "    \n",
    "    return focus_vs_distance_dir, plots_created\n",
    "\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "# Note: You need to define csv_files and global_ymax before running this section\n",
    "\n",
    "# Find common base directory for all outputs\n",
    "base_directory = find_common_base_directory(csv_files)\n",
    "print(f\"\\nBase directory for outputs: {base_directory}\")\n",
    "\n",
    "# Calculate Loss modulus global y-max\n",
    "global_ymax_loss = 0\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, skiprows=53, header=None)\n",
    "        loss_data = df.iloc[0:, [0, 7]].dropna()\n",
    "        loss_data.columns = ['Angular Frequency', 'Loss Modulus']\n",
    "        loss_data = loss_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "        \n",
    "        if not loss_data.empty:\n",
    "            freq_mask = (loss_data['Angular Frequency'] >= 0.3) & (loss_data['Angular Frequency'] <= 1.0)\n",
    "            if freq_mask.any():\n",
    "                max_val = loss_data.loc[freq_mask, 'Loss Modulus'].mean()\n",
    "                global_ymax_loss = max(global_ymax_loss, max_val)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nGlobal Y-max Storage Modulus = {global_ymax:.1f} Pa\")\n",
    "print(f\"Global Y-max Loss Modulus = {global_ymax_loss:.1f} Pa\")\n",
    "\n",
    "# Check if we have Focus/Distance files\n",
    "focus_files = [f for f in csv_files if detect_focus_distance_filename(f) == 'Focus']\n",
    "distance_files = [f for f in csv_files if detect_focus_distance_filename(f) == 'Distance']\n",
    "\n",
    "if focus_files or distance_files:\n",
    "    print(f\"\\nDetected Focus/Distance naming convention\")\n",
    "    print(f\"Focus files: {len(focus_files)}, Distance files: {len(distance_files)}\")\n",
    "    print(\"Creating separate plots for Focus and Distance measurements\\n\")\n",
    "    \n",
    "    if focus_files:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CREATING FOCUS PLOTS\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        fd_output_dir, fd_plots, _ = plot_gelatin_organized_data(focus_files, global_ymax, global_ymax_loss, \n",
    "                                                               base_directory, add_dots=False, output_suffix=\"_focus\")\n",
    "        print(f\"\\nCreated {len(fd_plots)} Focus plots in: {os.path.relpath(fd_output_dir, base_directory)}\")\n",
    "    \n",
    "    if distance_files:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CREATING DISTANCE PLOTS (with dot pattern)\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        dist_output_dir, dist_plots, _ = plot_gelatin_organized_data(distance_files, global_ymax, global_ymax_loss,\n",
    "                                                                   base_directory, add_dots=True, output_suffix=\"_distance\")\n",
    "        print(f\"\\nCreated {len(dist_plots)} Distance plots with dot pattern in: {os.path.relpath(dist_output_dir, base_directory)}\")\n",
    "    \n",
    "    # ===== Create Focus vs Distance Comparison Charts =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING FOCUS VS DISTANCE COMPARISON CHARTS\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    fvd_dir, fvd_plots = create_focus_vs_distance_comparisons(csv_files, global_ymax, global_ymax_loss, base_directory)\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo Focus/Distance naming detected - processing all files together\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"CREATING PLOTS FOR ALL DATA\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    output_dir, plots, _ = plot_gelatin_organized_data(csv_files, global_ymax, global_ymax_loss, \n",
    "                                                     base_directory, add_dots=False, output_suffix=\"\")\n",
    "    print(f\"\\nCreated {len(plots)} plots in: {os.path.relpath(output_dir, base_directory)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BATCH PROCESSING COMPLETE!\")\n",
    "print(f\"Total CSV files processed: {len(csv_files)}\")\n",
    "if focus_files or distance_files:\n",
    "    print(f\"Focus vs Distance comparison plots created: {len(fvd_plots)}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâœ“ Check the output folders for plots and statistical results!\")\n",
    "print(\"âœ“ Statistical test results saved in text files\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTPUT FOLDER STRUCTURE:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“ {base_directory}/\")\n",
    "print(\"   â”œâ”€â”€ ðŸ“ plots_by_gelatin_focus/\")\n",
    "print(\"   â”œâ”€â”€ ðŸ“ plots_by_gelatin_distance/\")\n",
    "print(\"   â””â”€â”€ ðŸ“ focus_vs_distance/\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ddf39-8dea-4300-b452-fc2e9c91056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === List All Sheet Names in Excel Files ===\n",
    "# Run this after Block 2 to see all sheet names in your Excel files\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXCEL FILE SHEET NAMES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, excel_file in enumerate(excel_files, 1):\n",
    "    rel_path = os.path.relpath(excel_file, base_directory)\n",
    "    print(f\"\\n{i}. {rel_path}\")\n",
    "    print(\"-\" * len(f\"{i}. {rel_path}\"))\n",
    "    \n",
    "    try:\n",
    "        xls = pd.ExcelFile(excel_file)\n",
    "        sheet_names = xls.sheet_names\n",
    "        \n",
    "        for j, sheet in enumerate(sheet_names, 1):\n",
    "            print(f\"   Sheet {j}: '{sheet}'\")\n",
    "            \n",
    "            # Show what the code detects in this sheet name\n",
    "            lower = sheet.lower()\n",
    "            detections = []\n",
    "            \n",
    "            # Check for cycles\n",
    "            if '0 cycles' in lower:\n",
    "                detections.append(\"0 cycles (0%)\")\n",
    "            if '40 cycles' in lower:\n",
    "                detections.append(\"40 cycles (3.6%)\")\n",
    "                \n",
    "            # Check for particles\n",
    "            if 'dbpc hmsn' in lower or 'dbpc_hmsn' in lower:\n",
    "                detections.append(\"DBPC HMSN\")\n",
    "            elif 'msn' in lower and 'hmsn' not in lower:\n",
    "                detections.append(\"MSN\")\n",
    "            elif '1x pbs' in lower or '1xpbs' in lower or 'pbs' in lower:\n",
    "                detections.append(\"PBS\")\n",
    "                \n",
    "            if detections:\n",
    "                print(f\"      â†’ Detected: {', '.join(detections)}\")\n",
    "            else:\n",
    "                print(f\"      â†’ No matches (will be skipped)\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR reading file: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Total Excel files examined: {len(excel_files)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9c6a0-0d5b-41bd-8bf6-2bfa568983a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (particle_tracker_real)",
   "language": "python",
   "name": "particle_tracker_real"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
