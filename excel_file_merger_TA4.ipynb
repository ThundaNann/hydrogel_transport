{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c12fde-5d71-4ad0-b5e7-8ab535f7f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Excel file merge process...\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\DBPC HMSN\\0 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'DBPC HMSN', 'cycles': '0', 'gelatin_conc': '8'}\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\DBPC HMSN\\11 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'DBPC HMSN', 'cycles': '11', 'gelatin_conc': '8'}\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\DBPC HMSN\\28 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'DBPC HMSN', 'cycles': '28', 'gelatin_conc': '8'}\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\DBPC HMSN\\40 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'DBPC HMSN', 'cycles': '40', 'gelatin_conc': '8'}\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\DBPC HMSN\\5 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'DBPC HMSN', 'cycles': '5', 'gelatin_conc': '8'}\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\MSN\\0 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'MSN', 'cycles': '0', 'gelatin_conc': '8'}\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\MSN\\11 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'MSN', 'cycles': '11', 'gelatin_conc': '8'}\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\MSN\\28 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'MSN', 'cycles': '28', 'gelatin_conc': '8'}\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\MSN\\40 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'MSN', 'cycles': '40', 'gelatin_conc': '8'}\n",
      "Warning: Could not extract required metadata from intensity_range_summary.xlsx, skipping\n",
      "  Path: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\MSN\\5 cycles\\stitched_image\\metrics\\intensity_range_summary.xlsx\n",
      "  Extracted metadata: {'particle_type': 'MSN', 'cycles': '5', 'gelatin_conc': '8'}\n",
      "\n",
      "Found 60 files\n",
      "\n",
      "Files by concentration group:\n",
      "  8pct_gelatin: 60 files\n",
      "    DBPC HMSN - 0 cycles: 6 replicates\n",
      "    DBPC HMSN - 11 cycles: 6 replicates\n",
      "    DBPC HMSN - 28 cycles: 6 replicates\n",
      "    DBPC HMSN - 40 cycles: 6 replicates\n",
      "    DBPC HMSN - 5 cycles: 6 replicates\n",
      "    MSN - 0 cycles: 6 replicates\n",
      "    MSN - 11 cycles: 6 replicates\n",
      "    MSN - 28 cycles: 6 replicates\n",
      "    MSN - 40 cycles: 6 replicates\n",
      "    MSN - 5 cycles: 6 replicates\n",
      "\n",
      "Found sheets to process: ['Area Profile', 'Metrics', 'Width Profile', 'Height Profile']\n",
      "\n",
      "============================================================\n",
      "Processing sheet type: Area Profile\n",
      "============================================================\n",
      "\n",
      "Creating file for 8pct_gelatin...\n",
      "  Added sheet: D_G8_40C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R5 from replicate_5_metrics.xlsx\n",
      "✓ Created: Merged_Area_Profile_8pct_gelatin.xlsx (3 sheets)\n",
      "\n",
      "============================================================\n",
      "Processing sheet type: Metrics\n",
      "============================================================\n",
      "\n",
      "Creating file for 8pct_gelatin...\n",
      "  Added sheet: D_G8_0C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R6 from replicate_6_metrics.xlsx\n",
      "✓ Created: Merged_Metrics_8pct_gelatin.xlsx (60 sheets)\n",
      "\n",
      "============================================================\n",
      "Processing sheet type: Width Profile\n",
      "============================================================\n",
      "\n",
      "Creating file for 8pct_gelatin...\n",
      "  Added sheet: D_G8_0C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R6 from replicate_6_metrics.xlsx\n",
      "✓ Created: Merged_Width_Profile_8pct_gelatin.xlsx (60 sheets)\n",
      "\n",
      "============================================================\n",
      "Processing sheet type: Height Profile\n",
      "============================================================\n",
      "\n",
      "Creating file for 8pct_gelatin...\n",
      "  Added sheet: D_G8_0C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_0C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_5C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_11C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_28C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: D_G8_40C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_0C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_5C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_11C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_28C_R6 from replicate_6_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R1 from replicate_1_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R2 from replicate_2_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R3 from replicate_3_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R4 from replicate_4_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R5 from replicate_5_metrics.xlsx\n",
      "  Added sheet: M_G8_40C_R6 from replicate_6_metrics.xlsx\n",
      "✓ Created: Merged_Height_Profile_8pct_gelatin.xlsx (60 sheets)\n",
      "\n",
      "============================================================\n",
      "All files created in: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\merged_analyses\n",
      "============================================================\n",
      "\n",
      "✓ Process completed successfully!\n",
      "  Files saved to: C:/Users/Talaial Alina/OneDrive - UCB-O365/Courses/Year 1/Fall Semester Aug-Dec 2020/CHEN 5840 - Independent Study/Hydrogels/Gel microscopy/06JUL25/8% gelatin\\merged_analyses\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "import re\n",
    "from openpyxl.utils import get_column_letter\n",
    "import openpyxl  \n",
    "\n",
    "def convert_to_minutes(time_value, unit='min'):\n",
    "    \"\"\"Convert time values to minutes regardless of input unit\"\"\"\n",
    "    try:\n",
    "        if unit.lower() in ['sec', 's']:\n",
    "            return float(time_value) / 60\n",
    "        return float(time_value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def extract_metadata_from_filename(filename):\n",
    "    \"\"\"Extract metadata specifically from the filename pattern seen in the logs\"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # Extract cycles\n",
    "    cycles_match = re.search(r'(\\d+)\\s*cycles', filename)\n",
    "    if cycles_match:\n",
    "        metadata['cycles'] = cycles_match.group(1)\n",
    "    \n",
    "    # Extract replicate number\n",
    "    replicate_match = re.search(r'replicate_(\\d+)', filename)\n",
    "    if replicate_match:\n",
    "        metadata['replicate'] = replicate_match.group(1)\n",
    "    \n",
    "    # Extract gelatin concentration (if present)\n",
    "    concentration_match = re.search(r'(\\d+)%\\s*gelatin', filename)\n",
    "    if concentration_match:\n",
    "        metadata['gelatin_conc'] = concentration_match.group(1)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def get_particle_type_from_file_path(file_path):\n",
    "    \"\"\"Extract particle type from the directory path\"\"\"\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    \n",
    "    # Look for particle type: should be after gelatin concentration and particle concentration\n",
    "    for i, part in enumerate(path_parts):\n",
    "        if 'gelatin' in part.lower() and '%' in part:\n",
    "            # Check next few directories after gelatin\n",
    "            for j in range(i+1, min(i+4, len(path_parts))):\n",
    "                potential = path_parts[j].strip()\n",
    "                # Skip concentration folders\n",
    "                if re.match(r'^\\d+\\s*ug-ml', potential, re.IGNORECASE):\n",
    "                    continue\n",
    "                # Skip common non-particle folders\n",
    "                if potential.lower() in ['stitched_image', 'metrics', 'merged_analyses']:\n",
    "                    continue\n",
    "                if 'replicate' in potential.lower():\n",
    "                    continue\n",
    "                if re.match(r'\\d+-\\d+;\\s*\\d+\\s*cycles', potential):  # Pattern like \"1-2; 28 cycles\"\n",
    "                    continue\n",
    "                # This looks like a particle type\n",
    "                return potential\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "def get_particle_prefix(particle_type):\n",
    "    \"\"\"Get single letter prefix for particle type\"\"\"\n",
    "    particle_upper = particle_type.upper()\n",
    "    if 'DBPC' in particle_upper and 'HMSN' in particle_upper:\n",
    "        return 'D'\n",
    "    elif particle_upper == 'MSN':\n",
    "        return 'M'\n",
    "    elif 'HMSN' in particle_upper:\n",
    "        return 'H'\n",
    "    else:\n",
    "        return 'U'  # Unknown\n",
    "\n",
    "def extract_metadata(file_path):\n",
    "    \"\"\"Extract metadata from the file path including both gelatin and particle concentrations\"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    \n",
    "    # First try to extract from filename directly\n",
    "    metadata = extract_metadata_from_filename(filename)\n",
    "    \n",
    "    # Extract particle type\n",
    "    metadata['particle_type'] = get_particle_type_from_file_path(file_path)\n",
    "    \n",
    "    # Extract cycles from path or filename\n",
    "    if 'cycles' not in metadata:\n",
    "        for part in path_parts + [filename]:\n",
    "            cycles_match = re.search(r'(\\d+)\\s*cycles', part.lower())\n",
    "            if cycles_match:\n",
    "                metadata['cycles'] = cycles_match.group(1)\n",
    "                break\n",
    "    \n",
    "    # Extract replicate if not already found\n",
    "    if 'replicate' not in metadata:\n",
    "        for part in path_parts:\n",
    "            if 'replicate' in part.lower():\n",
    "                try:\n",
    "                    replicate = part.split('replicate_')[1].split('_')[0]\n",
    "                    metadata['replicate'] = replicate\n",
    "                except IndexError:\n",
    "                    replicate = ''.join(filter(str.isdigit, part))\n",
    "                    if replicate:\n",
    "                        metadata['replicate'] = replicate\n",
    "                break\n",
    "        \n",
    "        if 'replicate' not in metadata:\n",
    "            rep_match = re.search(r'rep_(\\d+)', filename)\n",
    "            if rep_match:\n",
    "                metadata['replicate'] = rep_match.group(1)\n",
    "    \n",
    "    # Extract GELATIN concentration from path (look for \"X% gelatin\")\n",
    "    if 'gelatin_conc' not in metadata:\n",
    "        for part in path_parts:\n",
    "            gelatin_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*%\\s*gelatin', part.lower())\n",
    "            if gelatin_match:\n",
    "                metadata['gelatin_conc'] = gelatin_match.group(1)\n",
    "                break\n",
    "    \n",
    "    # Extract PARTICLE concentration (ug-mL format)\n",
    "    for part in path_parts:\n",
    "        particle_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*ug-ml', part.lower())\n",
    "        if particle_match:\n",
    "            metadata['particle_conc'] = particle_match.group(1)\n",
    "            break\n",
    "    \n",
    "    # Extract treatment time\n",
    "    for part in path_parts:\n",
    "        if 'min' in part.lower():\n",
    "            try:\n",
    "                time_str = part.lower().split('min')[0].strip()\n",
    "                time_val = convert_to_minutes(time_str, 'min')\n",
    "                if time_val is not None:\n",
    "                    metadata['time'] = str(time_val)\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        elif 's' in part.lower() and part.lower().replace('s', '').strip().isdigit():\n",
    "            try:\n",
    "                time_str = part.lower().replace('s', '').strip()\n",
    "                time_val = convert_to_minutes(time_str, 's')\n",
    "                if time_val is not None:\n",
    "                    metadata['time'] = str(time_val)\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def create_sheet_name(metadata):\n",
    "    \"\"\"Create a sheet name based on available metadata - MAX 31 CHARS\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Particle type prefix (1 char: D, M, H, U)\n",
    "    if 'particle_type' in metadata:\n",
    "        prefix = get_particle_prefix(metadata['particle_type'])\n",
    "        parts.append(prefix)\n",
    "    \n",
    "    # Gelatin concentration (G# - short)\n",
    "    if 'gelatin_conc' in metadata:\n",
    "        parts.append(f\"G{metadata['gelatin_conc']}\")\n",
    "    \n",
    "    # Particle concentration (P# - short)\n",
    "    if 'particle_conc' in metadata:\n",
    "        parts.append(f\"P{metadata['particle_conc']}\")\n",
    "    \n",
    "    # Cycles (e.g., 28C)\n",
    "    if 'cycles' in metadata:\n",
    "        parts.append(f\"{metadata['cycles']}C\")\n",
    "    \n",
    "    # Treatment time (optional - usually not present)\n",
    "    if 'time' in metadata:\n",
    "        parts.append(f\"{metadata['time']}m\")\n",
    "    \n",
    "    # Replicate (R#)\n",
    "    if 'replicate' in metadata:\n",
    "        parts.append(f\"R{metadata['replicate']}\")\n",
    "    \n",
    "    sheet_name = '_'.join(parts) if parts else 'Unknown'\n",
    "    \n",
    "    # Ensure under 31 character limit\n",
    "    if len(sheet_name) > 31:\n",
    "        sheet_name = sheet_name[:31]\n",
    "        print(f\"Warning: Sheet name truncated to 31 chars: {sheet_name}\")\n",
    "    \n",
    "    return sheet_name\n",
    "\n",
    "def create_grouping_key(metadata):\n",
    "    \"\"\"Create a key for grouping files (by gelatin and particle concentration)\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    if 'gelatin_conc' in metadata:\n",
    "        parts.append(f\"{metadata['gelatin_conc']}pct_gelatin\")\n",
    "    \n",
    "    if 'particle_conc' in metadata:\n",
    "        parts.append(f\"{metadata['particle_conc']}ugmL\")\n",
    "    \n",
    "    return '_'.join(parts) if parts else 'ungrouped'\n",
    "\n",
    "def find_excel_files(directory):\n",
    "    excel_files = []\n",
    "    unique_files = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Filter out directories to skip\n",
    "        dirs_to_remove = []\n",
    "        for d in dirs:\n",
    "            if 'blank' in d.lower() or 'merged_analyses' in d.lower():\n",
    "                dirs_to_remove.append(d)\n",
    "        \n",
    "        for d in dirs_to_remove:\n",
    "            if d in dirs:\n",
    "                dirs.remove(d)\n",
    "        \n",
    "        for file in files:\n",
    "            if ('metrics' in root or 'metrics' in file.lower()) and \\\n",
    "               file.endswith('.xlsx') and \\\n",
    "               not file.startswith('~') and \\\n",
    "               'blank_statistics' not in file and \\\n",
    "               'blank' not in root.lower():\n",
    "                \n",
    "                full_path = os.path.join(root, file)\n",
    "                metadata = extract_metadata(full_path)\n",
    "                \n",
    "                # Only process if we have required metadata\n",
    "                if metadata.get('cycles') is not None and metadata.get('replicate') is not None:\n",
    "                    file_key = full_path\n",
    "                    \n",
    "                    if file_key not in unique_files:\n",
    "                        unique_files[file_key] = (full_path, metadata)\n",
    "                    else:\n",
    "                        print(f\"Skipping duplicate file: {os.path.basename(full_path)}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Could not extract required metadata from {file}, skipping\")\n",
    "                    print(f\"  Path: {full_path}\")\n",
    "                    print(f\"  Extracted metadata: {metadata}\")\n",
    "    \n",
    "    for file_key, (file_path, metadata) in unique_files.items():\n",
    "        excel_files.append((file_path, metadata))\n",
    "    \n",
    "    # Sort files by particle type, gelatin conc, particle conc, cycles, and replicate\n",
    "    excel_files.sort(key=lambda x: (\n",
    "        x[1].get('particle_type', 'Unknown'),\n",
    "        float(x[1].get('gelatin_conc', '0')),\n",
    "        float(x[1].get('particle_conc', '0')),\n",
    "        int(x[1]['cycles']),\n",
    "        int(x[1]['replicate'])\n",
    "    ))\n",
    "    \n",
    "    return excel_files\n",
    "\n",
    "def autofit_columns(worksheet):\n",
    "    \"\"\"Auto-fit columns in a worksheet\"\"\"\n",
    "    for column in worksheet.columns:\n",
    "        max_length = 0\n",
    "        column_letter = get_column_letter(column[0].column)\n",
    "        \n",
    "        for cell in column:\n",
    "            try:\n",
    "                if cell.value:\n",
    "                    max_length = max(max_length, len(str(cell.value)))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        adjusted_width = min(max_length + 2, 50)\n",
    "        if adjusted_width > 0:\n",
    "            worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "\n",
    "def merge_excel_files(merge_metrics=True, group_by_concentration=True):\n",
    "    \"\"\"\n",
    "    Main function to merge Excel files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    merge_metrics : bool, optional\n",
    "        Whether to merge Metrics sheets (default True)\n",
    "    group_by_concentration : bool, optional\n",
    "        Whether to create separate merged files for each concentration combination (default True)\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes('-topmost', True)\n",
    "    \n",
    "    selected_dir = filedialog.askdirectory(title=\"Select Directory\")\n",
    "    root.attributes('-topmost', False)\n",
    "    \n",
    "    if not selected_dir:\n",
    "        print(\"No directory selected\")\n",
    "        return\n",
    "\n",
    "    excel_files = find_excel_files(selected_dir)\n",
    "    if not excel_files:\n",
    "        print(\"No Excel files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(excel_files)} files\")\n",
    "    \n",
    "    if not merge_metrics:\n",
    "        print(\"*** Metrics sheets will be SKIPPED ***\")\n",
    "    \n",
    "    # Print summary of found files grouped by concentrations\n",
    "    conc_groups = {}\n",
    "    for _, metadata in excel_files:\n",
    "        key = create_grouping_key(metadata)\n",
    "        if key not in conc_groups:\n",
    "            conc_groups[key] = []\n",
    "        conc_groups[key].append(metadata)\n",
    "    \n",
    "    print(\"\\nFiles by concentration group:\")\n",
    "    for group_key, files in conc_groups.items():\n",
    "        print(f\"  {group_key}: {len(files)} files\")\n",
    "        # Show particle type and cycles breakdown\n",
    "        particle_counts = {}\n",
    "        for meta in files:\n",
    "            particle = meta.get('particle_type', 'Unknown')\n",
    "            cycles = meta.get('cycles', 'Unknown')\n",
    "            key = f\"{particle} - {cycles} cycles\"\n",
    "            particle_counts[key] = particle_counts.get(key, 0) + 1\n",
    "        for key, count in sorted(particle_counts.items()):\n",
    "            print(f\"    {key}: {count} replicates\")\n",
    "    \n",
    "    # Create the merged_analyses folder\n",
    "    save_dir = os.path.join(selected_dir, 'merged_analyses')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Get all possible sheet names\n",
    "    all_sheets = set()\n",
    "    for file_path, _ in excel_files:\n",
    "        try:\n",
    "            file_sheets = pd.ExcelFile(file_path).sheet_names\n",
    "            all_sheets.update(file_sheets)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading sheets from {os.path.basename(file_path)}: {e}\")\n",
    "    \n",
    "    if not merge_metrics:\n",
    "        all_sheets = {sheet for sheet in all_sheets if sheet.lower() != 'metrics'}\n",
    "    \n",
    "    sheet_names = list(all_sheets)\n",
    "    print(f\"\\nFound sheets to process: {sheet_names}\")\n",
    "\n",
    "    # Process each sheet type\n",
    "    for sheet_type in sheet_names:\n",
    "        if not merge_metrics and sheet_type.lower() == 'metrics':\n",
    "            continue\n",
    "        \n",
    "        sheet_dir = os.path.join(save_dir, sheet_type.replace(\" \", \"_\"))\n",
    "        if not os.path.exists(sheet_dir):\n",
    "            os.makedirs(sheet_dir)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing sheet type: {sheet_type}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if group_by_concentration:\n",
    "            # Create separate files for each concentration combination\n",
    "            for group_key in conc_groups.keys():\n",
    "                group_files = [(fp, meta) for fp, meta in excel_files \n",
    "                              if create_grouping_key(meta) == group_key]\n",
    "                \n",
    "                if not group_files:\n",
    "                    continue\n",
    "                \n",
    "                output_file = os.path.join(sheet_dir, \n",
    "                                          f'Merged_{sheet_type.replace(\" \", \"_\")}_{group_key}.xlsx')\n",
    "                \n",
    "                print(f\"\\nCreating file for {group_key}...\")\n",
    "                \n",
    "                with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                    sheets_written = 0\n",
    "                    \n",
    "                    for file_path, metadata in group_files:\n",
    "                        try:\n",
    "                            sheet_name = create_sheet_name(metadata)\n",
    "                            \n",
    "                            try:\n",
    "                                df = pd.read_excel(file_path, sheet_name=sheet_type)\n",
    "                                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                                sheets_written += 1\n",
    "                                print(f\"  Added sheet: {sheet_name} from {os.path.basename(file_path)}\")\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"  Error processing {os.path.basename(file_path)}: {e}\")\n",
    "                    \n",
    "                    if sheets_written > 0 and hasattr(writer, 'book'):\n",
    "                        workbook = writer.book\n",
    "                        for worksheet in workbook.worksheets:\n",
    "                            autofit_columns(worksheet)\n",
    "                \n",
    "                if sheets_written > 0:\n",
    "                    print(f\"✓ Created: {os.path.basename(output_file)} ({sheets_written} sheets)\")\n",
    "                else:\n",
    "                    try:\n",
    "                        os.remove(output_file)\n",
    "                    except:\n",
    "                        pass\n",
    "        else:\n",
    "            # Create one file with all data\n",
    "            all_output_file = os.path.join(sheet_dir, f'Merged_{sheet_type.replace(\" \", \"_\")}.xlsx')\n",
    "            \n",
    "            with pd.ExcelWriter(all_output_file, engine='openpyxl') as writer:\n",
    "                sheets_written = 0\n",
    "                \n",
    "                for file_path, metadata in excel_files:\n",
    "                    try:\n",
    "                        sheet_name = create_sheet_name(metadata)\n",
    "                        \n",
    "                        try:\n",
    "                            df = pd.read_excel(file_path, sheet_name=sheet_type)\n",
    "                            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                            sheets_written += 1\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {os.path.basename(file_path)}: {e}\")\n",
    "                \n",
    "                if sheets_written > 0 and hasattr(writer, 'book'):\n",
    "                    workbook = writer.book\n",
    "                    for worksheet in workbook.worksheets:\n",
    "                        autofit_columns(worksheet)\n",
    "            \n",
    "            if sheets_written > 0:\n",
    "                print(f\"Created merged file: {all_output_file}\")\n",
    "            else:\n",
    "                try:\n",
    "                    os.remove(all_output_file)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"All files created in: {save_dir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    root.destroy()\n",
    "    return save_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Excel file merge process...\")\n",
    "    \n",
    "    # Group by concentration (creates separate files for each gelatin/particle combo)\n",
    "    save_dir = merge_excel_files(merge_metrics=True, group_by_concentration=True)\n",
    "    \n",
    "    # To merge everything into one file regardless of concentration:\n",
    "    # save_dir = merge_excel_files(merge_metrics=True, group_by_concentration=False)\n",
    "    \n",
    "    if save_dir:\n",
    "        print(f\"\\n✓ Process completed successfully!\")\n",
    "        print(f\"  Files saved to: {save_dir}\")\n",
    "    else:\n",
    "        print(\"Process was cancelled or encountered an error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322dcd0-1645-40e7-b150-2eb4cc26626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (particle_tracker_real)",
   "language": "python",
   "name": "particle_tracker_real"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
